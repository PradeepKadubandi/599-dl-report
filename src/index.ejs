<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style>
    .l-body {
      text-align: center;
    }
  </style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Learning a Sentence Embedding by Actuation",
  "description": "CSCI 599 Course Project Final Report",
  "authors": [
    {
      "author": "Team DeepJohn",
      "authorURL": "https://github.com/zhanpenghe/embed2learn/",
      "affiliation": "University of Southern California",
      "affiliationURL": "https://www.cs.usc.edu/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>

<d-article>

  <h2>
    Goal
  </h2>
  <p>
    In recent years there has been great progress in algorithms in the 
    field of deep reinforcement learning (DRL). These algorithms have 
    been applied to learning complex and diverse skills from raw observations.
    Among these successes, learning representations of tasks in the 
    form of latent space is demonstrated to be useful for learning 
    composable skills <d-cite key="hausman2018learning"></d-cite>.
    While skills can be defined discretely with labels and used for planning, 
    humans tend to use natural language to describe a task.
    For example, people can ask "Take a cup from shelf, fill in water, 
    and give it to me." Such natural language instructions provide an 
    expressive way to describe complex tasks using skill primitives 
    and allow the human to be in the loop to assist an agent.
  </p>

  <h2>
    Motivation
  </h2>
  <p>
    We propose a novel method that learns an embedding function that 
    embeds natural language sentences and their composability properties 
    (eg. concatenation) to a continuous latent space.
    Jointly, our approach also learns a policy that conditions on a latent 
    variable and state information with reinforcement learning and 
    variational inference. 
    We will explore the property of our latent space and utilize it 
    as a planning tool for complicated tasks. We will scope our work to
    achieve sequencing property of language i.e., concatenating simple tasks 
    to achieve more complex tasks.
  </p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/teaser.png" style="width: 80%">
    </div>
    <figcaption>
      <span class="figure-number">Fig 1. Concatenating simple tasks to describe
        a complex task.</span>
    </figcaption>
  </figure>

  <h2>Related Work</h2>
  <p>
      <b>Language-Guided Agent</b> There has been extensive studies done in the
      language-guided agent domain 
      <d-cite key="macmahon2006walk"></d-cite>, <d-cite key="branavan2009reinforcement"></d-cite>, 
      <d-cite key="misra2017mapping"></d-cite>, <d-cite key="latent_language_2018"></d-cite>, 
      <d-cite key="coreyes2018guiding"></d-cite>.
      Most of these works either learn a policy directly from state 
      and instruction representation or learn an intermediate representation 
      of goal and utilize it for planning.
      In <d-cite key="oh2017zeroshot"></d-cite>, task embedding is used and 
      conditioned on a policy for instruction following, but the instructions 
      used in this work was defined by categorical task parameters rather 
      than natural language. Our work not only explores using natural language 
      as task descriptions but explicitly impose property for planning on 
      our sentence embedding space.
  </p>

  <p><b>Task Embedding</b> Task embedding has been explored by 
    <d-cite key="hausman2018learning"></d-cite>, which learns a mapping from 
    task ID to a continuous space. However, sampling from this embedding 
    function is hard because it takes task ID's, which need to be defined 
    before the pre-training process, as input. 
    Our work utilizes the flexibility of natural languages. This allows us 
    to further utilize our latent space for learning from demonstration 
    and generating trajectory descriptions.
  </p>

  <p>
    <b>Sentence Embedding</b> Sentence embeddings is studied extensively 
    by many researchers. For example, <d-cite key="conneauInferSent"></d-cite> 
    learns a semantic representation by natural language inference. 
    Our work focuses on learning representations to aid the agent to 
    make decisions. Exploiting the advantages of interacting with the 
    environment, our methods should learn a minimal representation of 
    natural language commands and generalize among similar sentences.
  </p>

  <h2>Problem Formulation</h2>
  <p>
      In our multi-task RL setting, we pre-define a set of low-level skills 
      with their descriptions $T = \{ st_1, ..., st_N \} \subset ST$, 
      and accompanying, per-skill reward functions $r_{st \in T}(s, a)$. 
      We would like to learn a controller that takes both state information 
      and natural language commands into account. We also want to learn 
      an embedding function from which we can sample efficiently.
  </p>

  <h2>Models and Approaches</h2>
  <p>
      Our method, as illustrated in Figure 2, is motivated by 
      <d-cite key="hausman2018learning"></d-cite>. The model we design consists 
      of three components: the task embedding network $p_{\phi}: ST \to Z$, 
      the task embedding conditioned policy $\pi_{\theta}: S \times Z \to A$, 
      and the inference network $q_\psi: (S \times A)^H \to Z$.
  </p>

  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/method_improved.png" style="width: 70%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 2. Method</span>
    </figcaption>
  </figure>

  <p>
      Given a sentence $st$ that describes the task, the task embedding 
      network $p_\phi$ encodes the sentence to an embedding $z$. 
      We model $p_\phi$ by an embedding lookup layer followed by 
      a LSTM layer and a multi-layer perceptron with two heads in 
      the last layer to extract information from the natural language 
      task description and output a distribution for task embedding $z$.
      For simple experiments in the point mass experiment with a small 
      corpus, we train the task embedding network from scratch; for more 
      complicated experiments with large corpus, we pre-train the embedding 
      lookup layer with NCE loss and then use the pretrained vector 
      representations of words for further training of the model.
  </p>

  <p>
      With the task embedding $z$, the agent executes action sampled 
      from $\pi_\theta$ to obtain action $a_i$ at given state $s_i$.
      We model the policy network as a two-layer MLP with two-heads 
      representing the mean and covariance of the distribution from 
      which actions can be sampled.
  </p>

  <p>
      To aid in learning the embedding function, we learn an inference 
      function $q_\psi$ which, given a trajectory window $s_i^H$ of 
      length $H$, predicts the latent vector $z$ which was fed to the 
      low-level skill policy when it produced that trajectory.
      This allows us to define an augmented reward which encourages 
      the policy to produce distinct trajectories for different latent 
      vectors. We learn $q_\psi$ in parallel with the policy and 
      embedding functions. We model the inference network as a two-layer MLP.
  </p>

  <p>
      In order to learn a latent space that is meaningful for planning, 
      we would like to enforce some properties on our latent space.
      For two commands $st_1$ and $st_2$, sending a concatenation of 
      them to our low level controller should express behaviors of 
      doing $st_1$ then $st_2$. Hence, we approach this by augmenting 
      our RL objective with a regularization term, as shown below:
  </p>

  <figure>
    <div class="l-body">
      <%= require("../static/images/eq1.svg") %>
    </div>
  </figure>
  <p>where</p>
  <figure>
    <div class="l-body">
      <%= require("../static/images/eq2.svg") %>
    </div>
  </figure>
  <p>and</p>
  <figure>
    <div class="l-body">
      <%= require("../static/images/eq3.svg") %>
    </div>
  </figure>

  <p>Our KL Divergence regularization term is pictorially shown below in Figure 3:</p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/kl.png" style="width: 70%">
    </div>
    <figcaption>
      <span class="figure-number">Fig 3. KL Divergence</span>
    </figcaption>
  </figure>

  <p>
      In order to calculate the regularization term, we augment our 
      sampled data from the environment with concatenated trajectories.
      However, according to our experiments, concatenated trajectories 
      can lead to noises for reinforcement learning. Hence, we dynamically 
      adjust the coefficient of the regularization term according to 
      the distance of the last observation of the first trajectory 
      and the first observation of the second trajectory as shown in Figure 4.
  </p>

  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/hindsight_data.png" style="width: 65%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 4. Hindsight Experiene Replay with Adaptive Learning Rate.</span>
    </figcaption>
  </figure>

  <p>Our complete algorithm is shown below in Figure 5.</p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/algorithm.png" style="width: 90%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 5. Algorithm</span>
    </figcaption>
  </figure>

  <h2>Experiments</h2>
  <p>
      We setup a point mass environment with natural language input 
      to show that our method can learn task embeddings capable of 
      finishing a distribution of different tasks. The environment 
      has 4 different tasks, asking the point mass agent to move 
      right, up, left, and down to a goal position that is 3 units 
      away from the agent's start position. The agent's start position 
      is fixed at the origin of the space. The natural language 
      instructions are hardcoded for the simplicity of experiment setup.
      The agent get a dense reward that is inversely proportional 
      to the distance between the agent and the target location.
  </p>

  <p>
    As shown in Fig.6 below, the trajectories generated by policy trained
    with regularization term are more diverse compared to trajectories
    generated from the policy trained without the regularization term.
  </p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/multitasks.png">
    </div>
    <figcaption>
      <span class="figure-number">Figure 6. Left: Trajectories from 
        policy without regularization. Right: With regularization.</span>
    </figcaption>
  </figure>

  <p>
    Also, when we used the regularization, we can see that the learned
    task embeddings enable the policy to achive more complex goal
    (specifically a goal described as a concatenated sentence from simpler goals).
    As shown in the Fig.7 below, when we used a complex goal at test time,
    the policy network trained only with simpler goals at training time,
    successfully completes the complex task by sequencing the simpler sub-tasks.
    The policy without regularization term fails to achieve this.
  </p>

  <figure> 
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/rollout.png" style="width: 50%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 7. Trajectories from concatenated task.
      </span>
    </figcaption>
  </figure>

  <h2>Future Work</h2>
  <p>
    <ul>
      <li>Run the experiments in a more complicated and realistic environment.</li>
      <li>Build a more general purpose language instructions to run experiments.</li>
      <li>Running experiements on actual robots.</li>
    </ul>
  </p>

  <h2>Conclusion</h2>
  <p>
      <ul>
        <li>Our model can effectively learn a task embedding that 
          represents diverse skills.</li>
        <li>When adding the sequencing regularization term, the learned 
          task embedding supports sequencing of two tasks by 
          receiving previously unseen concatenated task descriptions.</li>
        <li>The transition of two tasks are smooth but might not 
          achieve each of the two task components perfectly.</li>
      </ul>
  </p>
</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to Joseph J. Lim, Karl Pertsch, without the help of whom this project would not have happened.
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Zhanpeng He</b> provided the template code for the project, implemented KL term and hindsight data collection algorithm, and worked on making adjustments of the model to make training work.
  </p>
  <p>
    <b>Jingyun Yang</b> implemented sentence embedding and KL term introduced in the work; he also worked on making model figures and illustrations.
  </p>
  <p>
    <b>Pradeep Kadubandi</b> explored various experiment settings for our model and composed the final report of the project.
  </p>
  <p>
    <b>Yao Gu</b> worked on the point mass environment on which we conducted all our experiments and developed part of our visualization code.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
