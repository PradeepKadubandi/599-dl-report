<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style>
    .div-bg {
      background: url("https://wallpaperaccess.com/full/400483.jpg") no-repeat center center fixed; 
      -webkit-background-size: cover;
      -moz-background-size: cover;
      -o-background-size: cover;
      background-size: cover;
      padding-top: 40px;
      padding-bottom: 50px;
    }
    .div-bg d-byline {
      border-top: none;
    }
    .div-bg h1, .div-bg a, .div-bg h3, .div-bg p {
      color: white;
      text-shadow: #000 0px 0px 5px;
    }
    .l-body {
      text-align: center;
    }
  </style>
  <style>
    .kernel-grid {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
    }
    .grid-item {
      margin: 1em;
    }
  </style>
</head>

<body>

<div class="div-bg">
<d-front-matter>
  <script type="text/json">{
  "title": "Learning a Sentence Embedding by Actuation",
  "description": "CSCI 599 Course Project Final Report",
  "authors": [
    {
      "author": "Zhanpeng He",
      "affiliation": "USC",
      "affiliationURL": "https://www.cs.usc.edu/"
    },
    {
      "author": "Jingyun Yang",
      "affiliation": "USC",
      "affiliationURL": "https://www.cs.usc.edu/"
    },
    {
      "author": "Pradeep Kadubandi",
      "affiliation": "USC",
      "affiliationURL": "https://www.cs.usc.edu/"
    },
    {
      "author": "Yao Gu",
      "affiliation": "USC",
      "affiliationURL": "https://www.cs.usc.edu/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title></d-title>
<d-byline></d-byline>
</div>

<d-article>

  <h2>
    Introduction
  </h2>
  <p>
    In recent years, there has been great progress in algorithms in the 
    field of deep reinforcement learning (DRL). These algorithms have 
    been applied to learning complex and diverse skills from raw observations.
    Among these successes, learning representations of tasks in the 
    form of latent space is demonstrated to be useful for learning 
    composable skills <d-cite key="hausman2018learning"></d-cite>.
  </p>
  <p>
    While skills can be defined discretely with labels and used for planning, 
    humans tend to use natural language to describe a task. In addition to the difference in task specification format, natural language instructions tend to be more flexible than discrete labels. For example, let's say you are playing an imitation game with your friend, in which you receive the instruction that reads "Jump up the terrain. Then, move forward," as shown in Figure 1. By receiving this instruction, you realize that to complete the instructed task, you need to first complete the "jumping up the terrain" task, then the "moving forward" task. This is because humans are capable of decomposing a task description of concatenated subtask descriptions into sequentially operatable components. Can autonomous agents recognize such properties in natural language instructions too?
  </p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/teaser.png" style="width: 80%">
    </div>
    <figcaption>
      <span class="figure-number">Fig 1. Concatenating simple tasks to describe
        a complex task.</span>
    </figcaption>
  </figure>
  <p>
    In this work, we propose a novel method that learns an embedding function that 
    embeds natural language sentences and their composability properties 
    (eg. concatenation) to a continuous latent space.
    Jointly, our approach also learns a policy that conditions on a latent 
    variable and state information with reinforcement learning and 
    variational inference. 
    We will explore the property of our latent space and utilize it 
    as a planning tool for complicated tasks. We will scope our work to
    achieve sequencing property of language i.e., concatenating simple tasks 
    to achieve more complex tasks.
  </p>

  <h2>Past Works</h2>
  <p>
      <b>Language-Guided Agent</b> There has been extensive studies done in the
      language-guided agent domain 
      <d-cite key="macmahon2006walk"></d-cite>, <d-cite key="branavan2009reinforcement"></d-cite>, 
      <d-cite key="misra2017mapping"></d-cite>, <d-cite key="latent_language_2018"></d-cite>, 
      <d-cite key="coreyes2018guiding"></d-cite>.
      Most of these works either learn a policy directly from state 
      and instruction representation or learn an intermediate representation 
      of goal and utilize it for planning.
      In <d-cite key="oh2017zeroshot"></d-cite>, task embedding is used and 
      conditioned on a policy for instruction following, but the instructions 
      used in this work was defined by categorical task parameters rather 
      than natural language. Our work not only explores using natural language 
      as task descriptions but explicitly impose property for planning on 
      our sentence embedding space.
  </p>

  <p><b>Task Embedding</b> Task embedding has been explored by 
    <d-cite key="hausman2018learning"></d-cite>, which learns a mapping from 
    discrete task labels to a continuous latent space. However, sampling from this embedding 
    function is hard because it takes task labels, which need to be defined 
    before the pre-training process, as input. In order to achieve unseen tasks, this system has to train a new model to sample latent variables, as <d-cite key="he2018zero"></d-cite> and <d-cite key="julian2018scaling"></d-cite> do. Our work utilizes the flexibility of natural languages. This allows us to further utilize our latent space for unseen task during pre-training. In comparison to <d-cite key="hausman2018learning"></d-cite>, our work also embeds the sequencing information to the latent space. Without further learning and environment interactions, we demonstrated the possibilities of learning multiple tasks and sequencing them in parallel. 
  </p>

  <p>
    <b>Sentence Embedding</b> Sentence embeddings is studied extensively 
    by many researchers. For example, <d-cite key="conneauInferSent"></d-cite> 
    learns a semantic representation by natural language inference. 
    Our work focuses on learning representations to aid the agent to 
    make decisions. Exploiting the advantages of interacting with the 
    environment, our methods should learn a minimal representation of 
    natural language commands and generalize among similar sentences.
  </p>

  <h2>Problem Formulation</h2>
  <p>
      In our multi-task RL setting, we pre-define a set of low-level skills 
      with their descriptions $T = \{ st_1, ..., st_N \} \subset ST$, 
      and accompanying, per-skill reward functions $r_{st \in T}(s, a)$. 
      We would like to learn a controller that takes both state information 
      and natural language commands into account. We also want to learn 
      an embedding function from which we can sample efficiently.
  </p>

  <h2>Approach</h2>
  <p>
      Our method, as illustrated in Figure 2, is motivated by 
      <d-cite key="hausman2018learning"></d-cite>. The model we design consists 
      of three components: the task embedding network $p_{\phi}: ST \to Z$, 
      the task embedding conditioned policy $\pi_{\theta}: S \times Z \to A$, 
      and the inference network $q_\psi: (S \times A)^H \to Z$.
  </p>

  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/method_improved.png" style="width: 70%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 2. Task Embedding Algorithm.</span>
    </figcaption>
  </figure>

  <p>
      Given a sentence $st$ that describes the task, the task embedding 
      network $p_\phi$ encodes the sentence to an embedding $z$. 
      We model $p_\phi$ by an embedding lookup layer followed by 
      a LSTM layer and a multi-layer perceptron with two heads in 
      the last layer to extract information from the natural language 
      task description and output a distribution for task embedding $z$.
      We train the task embedding network from scratch.
  </p>

  <p>
      With the task embedding $z$, the agent executes action sampled 
      from $\pi_\theta$ to obtain action $a_i$ at given state $s_i$.
      We model the policy network as a two-layer MLP with two-heads 
      representing the mean and covariance of the distribution from 
      which actions can be sampled.
  </p>

  <p>
      To aid in learning the embedding function, we learn an inference 
      function $q_\psi$ which, given a trajectory window $s_i^H$ of 
      length $H$, predicts the latent vector $z$ which was fed to the 
      low-level skill policy when it produced that trajectory.
      This allows us to define an augmented reward which encourages 
      the policy to produce distinct trajectories for different latent 
      vectors. We learn $q_\psi$ in parallel with the policy and 
      embedding functions. We model the inference network as a two-layer MLP.
  </p>

  <p>
      In order to learn a latent space that is meaningful for planning, 
      we added a regularization term to enforce the sequencing property on the latent space.
      Given two task descriptions $st_1$ and $st_2$, we require that the action distributions generated by execution the concatenation of the two task descriptions (ie. $[st_1, st_2]$) to be as similar as possible to the concatenated action distributions generated by executing each of the two task descriptions. The illustration of the regularization term we designed can be found below in Figure 3. 
  </p>

  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/kl.png" style="width: 85%">
    </div>
    <figcaption>
      <span class="figure-number">Fig 3. Visualization of the regularization term that we used in the objective function to minimize the KL-divergence between the action distributions produced by two single sentences and those produced by a concatenated sentence.</span>
    </figcaption>
  </figure>

  <p>
    The complete RL objective, augmented with the regularization term introduced above is shown below:
  </p>

  <figure>
    <div class="l-body">
      <%= require("../static/images/eq1.svg") %>
    </div>
  </figure>
  <p>where</p>
  <figure>
    <div class="l-body">
      <%= require("../static/images/eq2.svg") %>
    </div>
  </figure>
  <p>and</p>
  <figure>
    <div class="l-body">
      <%= require("../static/images/eq3.svg") %>
    </div>
  </figure>

  <p>
      In order to calculate the regularization term, we augment our 
      sampled data from the environment with concatenated trajectories.
      However, according to our experiments, concatenated trajectories 
      can lead to noises for reinforcement learning. Hence, we dynamically 
      adjust the coefficient of the regularization term according to 
      the distance of the last observation of the first trajectory 
      and the first observation of the second trajectory as shown in Figure 4.
  </p>

  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/hindsight_data.png" style="width: 65%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 4. Hindsight experiene replay with adaptive learning rate.</span>
    </figcaption>
  </figure>

  <p>Our complete algorithm is shown below in Algorithm 1.</p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/algorithm.png" style="width: 90%">
    </div>
    <figcaption>
      <span class="figure-number">Algorithm 1. Task-embedding with sequence regularization.</span>
    </figcaption>
  </figure>

  <h2>Results</h2>
  <p>
      We setup a point mass environment with natural language input 
      to show that our method can learn task embeddings capable of 
      achieving different tasks. In pre-train time, the environment 
      has 4 different tasks, asking the point mass agent to move 
      right, up, left, and down to a goal position that is 2 units 
      away from the agent's start position. The agent's start position 
      is fixed at the origin of the space. The natural language 
      instructions are hardcoded for the simplicity of experiment setup.
      The agent get a dense reward that is inversely proportional 
      to the distance between the agent and the target location.
  </p>

  <p>
    As shown in Figure 5 below, the trajectories generated by policy trained
    with regularization term are more diverse compared to trajectories
    generated from the policy trained without the regularization term.
  </p>
  <figure>
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/multitasks.png">
    </div>
    <figcaption>
      <span class="figure-number">Figure 5. Left: trajectories from 
        policy without regularization; right: with regularization.</span>
    </figcaption>
  </figure>

  <p>
    Also, when we used the regularization, we can see that the learned
    task embeddings enable the policy to achive more complex goal
    (specifically a goal described as a concatenated sentence from simpler goals).
    As shown in the Figure 6 below, when we used a complex goal at test time,
    the policy network trained only with simpler goals at training time,
    successfully completes the complex task by sequencing the simpler sub-tasks.
    The policy without regularization term fails to achieve this.
  </p>

  <figure> 
    <div class="l-body">
      <img src="https://raw.githubusercontent.com/PradeepKadubandi/599-dl-report/master/static/images/rollout.png" style="width: 80%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 6. Left: trajectory of rolling out of "move right and move up"; right: trajectory of rolling out of "move up and move right". 10 trajectories are collected for each sequencing tasks and the average of them are plotted.
      </span>
    </figcaption>
  </figure>
<!--
  <figure> 
    <div class="l-body">
      <img src="videos/point_reach_with_reg.png" style="width: 80%">
    </div>
    <figcaption>
      <span class="figure-number">Figure 8. Trajectory of rolling out of "reach green reach blue" with regularization.
      </span>
    </figcaption>
  </figure>

  <p></p>

  <p>
    
  </p>


  <p></p>
-->

  <h2>Future Work</h2>
  <p>
    For future work, we will run the experiments in a more complicated and realistic environment. The first environment we can think of is the minecraft environment, in which different types of objects are placed in a gridworld with different rules of interactions, and a task consists of making a specified interaction with the environment; the second environment is a realistic robot pushing envioronment where a task constitutes of pushing a described object in the scene to another specified location.
  </p>
  <p>
    Another task we will work on is scaling the language set that our experiment works with. Currently we are working on very small language sets with fixed grammar. We wish to collect a much larger task set with diverse language descriptions, possibly through human labeling of tasks.
  </p>

  <h2>Conclusion</h2>
  <p>
    We introduced a model that embeds natural language descriptions of tasks into a continuous latent space, which is conditioned on a policy for interaction with the environment. We introduced a regularization term for learning a sequencing property for the embedding space, as well as an algorithm for training the model with this regularization term.
  </p>
  <p>
    In our experiments, we show that our model can effectively learn a task embedding that represents diverse skills. We illustrate that when adding the sequencing regularization term, the learned task embedding supports sequencing of two tasks by receiving previously unseen concatenated task descriptions. As a limitation of our work, although the concatenated task description can be executed successfully and the transition of two tasks are smooth, the execution might not achieve each of the two task components perfectly.
  </p>
</d-article>



<d-appendix>
  <h3>Additional Resources</h3>
  <p>
    <a href="https://github.com/zhanpenghe/embed2learn"><b>Code:</b> zhanpenghe/embed2learn</a> <br>
    Open-source implementation of our project
  </p>
  <p>
    <a id="add" href="#add"><b>Appendix:</b> Additional Experiments</a> <br>
    In addition to the experiment results shown in the main blog, we also setup a robot reaching box environment using mujoco with natural language as input. In the training time, the environment has 4 different tasks that asking the robot arm to reach red, yellow, green or blue box. The agent's and boxes position are random. The agent will get a reward that is inversely proportional to the distance between agent and target box. The videos below show some of the results we get. From the videos, it can be seen that the trajectory of rolling out with regularization has sequencing property, while the trajectory without regularization does not.
    <div>
      <div class="kernel-grid">
        <div class="grid-item">
          <video controls width="250">
              <source src="videos/point_reach_without_reg.mov" type="video/mp4">
          </video>
        </div>
        <div class="grid-item">
          <video controls width="250">
            <source src="videos/point_reach_with_reg.mov" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <figcaption>
      <span class="figure-number">Figure 7. Left: trejectory of rolling out of "reach green reach blue" without regularization; right: trajectory of rolling out of "reach green reach blue" with regularization.
      </span>
    </figcaption>
  </p>

  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to Joseph J. Lim, Karl Pertsch, without the help of whom this project would not have happened.
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Zhanpeng He</b> provided the template code for the project, implemented KL term and hindsight data collection algorithm, and worked on making adjustments of the model to make training work.
  </p>
  <p>
    <b>Jingyun Yang</b> implemented sentence embedding and KL term introduced in the work; he also worked on making model figures and illustrations.
  </p>
  <p>
    <b>Pradeep Kadubandi</b> explored various experiment settings for our model 
    and composed the final report of the project.
    <a href="https://docs.google.com/document/d/1Sa7oKsFJAWvv0MaVvhr1EC99lXn7uA_c0yunXWiHQ1c/edit?usp=sharing">This</a> 
    document outlines a detailed report of various experiments attempted.
  </p>
  <p>
    <b>Yao Gu</b> worked on the point mass environment on which we conducted all our experiments and developed part of our visualization code.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript">
  $( document ).ready(function() {
    $('.authors-affiliations').next().children().last().children().last().text('April 30, 2019').css('font-style', 'normal');
    $('.authors-affiliations').next().next().children().first().text('Team Name').next().children().last().text('DeepJohn').css('font-style', 'normal');
  });
</script>
</body>
